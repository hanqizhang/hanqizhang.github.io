<!DOCTYPE html> <html lang="en-US"> <head prefix="og: http://ogp.me/ns#"> <meta charset="UTF-8" /> <meta http-equiv="X-UA-Compatible" content="ie=edge" /> <meta name="viewport" content="width=device-width, initial-scale=1.0" /> <meta name="mobile-web-app-capable" content="yes" /> <meta name="apple-mobile-web-app-capable" content="yes" /> <meta name="application-name" content="Plain neurotheory" /> <meta name="apple-mobile-web-app-status-bar-style" content="#fff" /> <meta name="apple-mobile-web-app-title" content="Plain neurotheory" /> <title> What does V1 know about orientations in natural scenes? - Plain neurotheory </title> <link rel="alternate" href="http://localhost:4000/efficient-coding-orien/" hreflang="en-US" /> <link rel="canonical" href="http://localhost:4000/efficient-coding-orien/" /> <meta name="description" content="Efficient coding for orientation." /> <meta name="referrer" content="no-referrer-when-downgrade" /> <meta property="fb:app_id" content="" /> <meta property="og:site_name" content="What does V1 know about orientations in natural scenes? | Hank Zhang" /> <meta property="og:title" content="What does V1 know about orientations in natural scenes? | Hank Zhang" /> <meta property="og:type" content="website" /> <meta property="og:url" content="http://localhost:4000/efficient-coding-orien/" /> <meta property="og:description" content="Efficient coding for orientation." /> <meta property="og:image" content="http://localhost:4000/assets/img/ogp.png" /> <meta property="og:image:width" content="640" /> <meta property="og:image:height" content="640" /> <meta name="twitter:card" content="summary" /> <meta name="twitter:title" content="What does V1 know about orientations in natural scenes? | twitter_username" /> <meta name="twitter:url" content="http://localhost:4000/efficient-coding-orien/" /> <meta name="twitter:site" content="@twitter_username" /> <meta name="twitter:creator" content="@twitter_username" /> <meta name="twitter:description" content="Efficient coding for orientation." /> <meta name="twitter:image" content="http://localhost:4000/assets/img/ogp.png" /> <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Plain neurotheory" /> <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicons/apple-touch-icon.png" /> <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png" /> <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png" /> <link rel="manifest" href="/assets/favicons/site.webmanifest" /> <link rel="mask-icon" href="/assets/favicons/safari-pinned-tab.svg" color="#5bbad5" /> <meta name="apple-mobile-web-app-title" content="Hank Zhang" /> <meta name="application-name" content="Hank Zhang" /> <meta name="msapplication-TileColor" content="#da532c" /> <meta name="theme-color" content="#2c2c2c" /> <link rel="stylesheet" href="/assets/css/style.css" /> <!-- for mathjax support --> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" }, tags: 'ams' } }); </script> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js?config=TeX-AMS-MML_HTMLorMML"></script> </head> <body data-theme="dark" class="notransition"> <script> const body = document.body; const data = body.getAttribute("data-theme"); const initTheme = (state) => { if (state === "dark") { body.setAttribute("data-theme", "dark"); } else if (state === "light") { body.removeAttribute("data-theme"); } else { localStorage.setItem("theme", data); } }; initTheme(localStorage.getItem("theme")); setTimeout(() => body.classList.remove("notransition"), 75); </script> <div class="navbar" role="navigation"> <nav class="menu"> <input type="checkbox" id="menu-trigger" class="menu-trigger" /> <label for="menu-trigger"> <span class="menu-icon"> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 512 512" > <path d="M64,384H448V341.33H64Zm0-106.67H448V234.67H64ZM64,128v42.67H448V128Z" /> </svg> </span> </label> <a id="mode"> <svg class="mode-sunny" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 512 512" > <title>LIGHT</title> <line x1="256" y1="48" x2="256" y2="96" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="256" y1="416" x2="256" y2="464" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="403.08" y1="108.92" x2="369.14" y2="142.86" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="142.86" y1="369.14" x2="108.92" y2="403.08" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="464" y1="256" x2="416" y2="256" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="96" y1="256" x2="48" y2="256" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="403.08" y1="403.08" x2="369.14" y2="369.14" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="142.86" y1="142.86" x2="108.92" y2="108.92" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <circle cx="256" cy="256" r="80" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> </svg> <svg class="mode-moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 512 512" > <title>DARK</title> <line x1="256" y1="48" x2="256" y2="96" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="256" y1="416" x2="256" y2="464" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="403.08" y1="108.92" x2="369.14" y2="142.86" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="142.86" y1="369.14" x2="108.92" y2="403.08" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="464" y1="256" x2="416" y2="256" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="96" y1="256" x2="48" y2="256" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="403.08" y1="403.08" x2="369.14" y2="369.14" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="142.86" y1="142.86" x2="108.92" y2="108.92" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <circle cx="256" cy="256" r="80" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> </svg> </a> <div class="trigger"> <div class="trigger-container"><a class="menu-link" href="/">home</a><a class="menu-link" href="/archive/">blogposts</a><a class="menu-link rss" href="/feed.xml"> <svg xmlns="http://www.w3.org/2000/svg" width="17" height="17" viewBox="0 0 512 512" fill="#ED812E" > <title>RSS</title> <path d="M108.56,342.78a60.34,60.34,0,1,0,60.56,60.44A60.63,60.63,0,0,0,108.56,342.78Z" /> <path d="M48,186.67v86.55c52,0,101.94,15.39,138.67,52.11s52,86.56,52,138.67h86.66C325.33,312.44,199.67,186.67,48,186.67Z" /> <path d="M48,48v86.56c185.25,0,329.22,144.08,329.22,329.44H464C464,234.66,277.67,48,48,48Z" /> </svg> </a> </div> </div> </nav> </div> <div class="wrapper post"> <main class="page-content" aria-label="Content"> <article itemscope itemtype="https://schema.org/BlogPosting"> <header class="header"> <div class="tags"> <span itemprop="keywords"> <a class="tag" href="/tags/#neuroscience">NEUROSCIENCE</a>, <a class="tag" href="/tags/#perception">PERCEPTION</a> </span> </div> <h1 class="header-title" itemprop="headline">What does V1 know about orientations in natural scenes?</h1> <div class="post-meta"> <time datetime="2024-06-23T00:00:00+08:00" itemprop="datePublished"> Jun 23, 2024 </time> <span itemprop="author" itemscope itemtype="https://schema.org/Person"> <span itemprop="name">Hank Zhang</span> </span> <time hidden datetime="2024-06-26T00:00:00+08:00" itemprop="dateModified"> Jun 23, 2024 </time> <span hidden itemprop="publisher" itemtype="Person">Hank Zhang</span> <span hidden itemprop="image"></span> <span hidden itemprop="mainEntityOfPage">Introduction and review of the efficient coding principle describing how the brain should allocate its neuronal resources to process natural images. Also some thoughts on how pathological situations could make the optimal solution not so pretty.</span> </div> </header> <div class="page-content" itemprop="articleBody"> <p>Horace Barlow put forth the efficient coding hypothesis in 1961 which has proved to be fundamentally useful for understanding and modeling how the brain encodes sensory information. So useful, in fact, that it is now often referred to as the efficient coding theory, or the efficient coding principle instead.</p> <p>The principle suggests that under strict resource constraints, our sensory system must have become near-optimal in maximally preserving sensory information given the statistics of naturally occurring stimuli. In plain words, with the limited neuronal resources we have, we must have learned (through evolution, development, or adaptation) to be efficient in allocating these neurons for processing what we typically see, hear, touch, smell, etc., <em>in a given environment</em>.</p> <p>Let’s start with an example from vision to demonstrate how this idea is formalized, and to make the principle concrete.</p> <p>Statistically speaking, in the natural environment, the orientations of local structures in an image are not uniformly distributed. More structures are cardinal (vertical/horizontal) than oblique (45/135 deg) (see, e.g., <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3125404/#SD1">Girschick et al</a>).</p> <figure> <p align="middle"> <img src="http://localhost:4000/assets/posts/efficient-coding-orien/orien-image.png" alt="orientation natural scene" width="40%" /> <img src="http://localhost:4000/assets/posts/efficient-coding-orien/orien-prior.png" alt="orientation distr" width="40%" /> </p> <figcaption>Fig 1. Orientation distribution in natural scenes (adapted from Girschick et al).</figcaption> </figure> <p>To maximize the encoding efficiency for orientation, the visual system should arrange its tuning curves to maximize the <a href="https://en.wikipedia.org/wiki/Mutual_information">mutual information</a> between the orientation \(\theta\) and the neural activity \(r\) encoding orientations. Intuitively speaking, high mutual information means that by observing the neural encoding of orientation, a decoder can obtain much information about the actual orientation, and <em>vice versa</em>.</p> <p>This mutual information quantity, written as \(I[\theta, r]\), is a function of the joint and marginal distributions of \(r\) and \(\theta\) and is closely related to the <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Shannon entropy</a> of each of the two variables (since entropy is also a function of a variable’s distribution). As it turns out, \(I[\theta, r]\) can also be written in terms of the Fisher Information (see <a href="https://hanqizhang.github.io/noise-and-fisher-info/">my previous post</a>, here a function of orientation) and the distribution of orientations in the environment: \(\begin{align} I[\theta, r] &amp;= I[f(\theta), r] = H[r] - \int{d\tilde{\theta}p(\tilde{\theta})H[r|\tilde{\theta}]}\label{eq1}\tag{1}\\ &amp;= H[r] - \int{d\tilde{\theta}p(\tilde{\theta})H[\delta]}\label{eq2}\tag{2}\\ &amp;= H[r] - \int{d\tilde{\theta}p(\tilde{\theta})\left(\frac{1}{2}\ln\frac{2\pi e}{J(\delta)}+D_0\right)}\label{eq3}\tag{3}\\ &amp;= (H[r] - H[\tilde{\theta}]) + H[\tilde{\theta}] - \frac{1}{2}\int{d\tilde{\theta}p(\tilde{\theta})\ln\frac{2\pi e}{J(\tilde{\theta})}} - D_0\label{eq4}\tag{4}\\ &amp;= H[\theta] - \frac{1}{2}\int{d\theta p(\theta)\ln\frac{2\pi e}{J(\theta)}} + C_0 - D_0\label{eq5}\tag{5}\\ &amp;= \frac{1}{2}\ln\frac{(\int{\sqrt{J(\theta)}d\theta})^2}{2\pi e} - KL\left(p(\theta)||\frac{\sqrt{J(\theta)}}{\int{\sqrt{J(\theta)}d\theta}}\right)+ C_0 - D_0 \label{eq6}\tag{6} \end{align}\)</p> <p>where: <br />     <strong>(i)</strong> \(g(\theta) = \tilde{\theta} = E[r|\theta]\) is the tuning function and assumed to be invertible.<br />     <strong>(ii)</strong> \(\delta\) in \eqref{eq3} is the additive noise (i.e., \(\delta = r - \tilde{\theta}\)) whose entropy, per <em>Stam’s Inequality</em>, is minimized <strong>iff</strong> \(\delta\sim\mathcal{N}(\mu,\sigma^2)\), where it also happens that \(J(\delta)=1/\sigma^2\). Thus, \(D_0\) is the entropy difference between its actual distribution and the entropy-minimizing Gaussian. <br />     <strong>(iii)</strong> \(C_0 = H[r] - H[\tilde{\theta}]\), where \(H(.)\) is the Shannon entropy.<br />     <strong>(iv)</strong> And finally, \(J(\tilde{\theta}) = J(\delta)\) since the \(p(r|\tilde{\theta})\) terms that appear in the definition of \(J(\tilde{\theta})\) are really just \(p(\delta)\).</p> <p>The entire derivation was provided in <a href="https://www.sas.upenn.edu/~astocker/lab/publications-files/journals/NC2016/Wei_Stocker2016.pdf">Wei &amp; Stocker’s 2016 paper</a>. Note that \(C_0\) and \(D_0\) do not matter asymptotically under vanishing Gaussian noise. And for the purpose of this post, let’s assume the Gaussianity of and size of noise is near that regime. Thus to maximize the whole expression is to minimize the KL term, which is achieved when \(p(\theta)\propto\sqrt{J(\theta)}\). In other words, orientation encoding is most efficient when the orientation sensitive neurons’ tuning profile achieves a Fisher Information profile that matches the squared distribution of orientations in the environment!</p> <p>One intuitive way to form such a tuning profile is simply to have more neurons tuned for (or sensitive to, or in charge of processing) cardinal orientations and fewer neurons for oblique orientations.</p> <p>Useful and intuitive as it is, the fact that this result boils down to the principle of <strong>“allocating more neurons for the more frequently encountered stimulus feature values”</strong> is a bit too reductive, but if you are new to efficient coding, <strong>this simple intuition would suffice as your main takeaway</strong>.</p> <p>If you are reading on, here are two lines of fine prints under the simple intuition. <strong>First of all</strong>, unintuitive tuning profiles abound for achieving the same FI profile. By parameterizing the tuning curves and training the parameters to minimize a mean-squared-error loss between the target and actual FI profiles, I can generate several distinctive tuning profiles all satisfying the same FI target profile. From those I picked 3 representative profiles to showcase the intricacy that belies the simple intuition of “more neurons for the more frequent orientations”.</p> <p>In figure 2 below, profiles 1 and 2 (identified in <a href="https://www.nature.com/articles/nn.4105">Wei &amp; Stocker 2015</a>) show that \(J(\theta)\) is not determined by the number of neurons preferring \(\theta\) but how many neurons have tuning curves crossing \(\theta\). Profile 3 demonstrates that tuning curve height does not necessarily drive up FI, although note that here I assume “multiplicative” Gaussian noise (Gaussian variance scales linearly with the expected firing rate), as opposed to Gaussian noise with a fixed variance.</p> <figure> <p align="middle"> <img src="http://localhost:4000/assets/posts/efficient-coding-orien/orientuning_0.png" width="32%" /> <img src="http://localhost:4000/assets/posts/efficient-coding-orien/orientuning_1.png" width="32%" /> <img src="http://localhost:4000/assets/posts/efficient-coding-orien/orientuning_3.png" width="32%" /> </p> <figcaption>Fig 2. Orientation tunings satisfying the efficient coding objective. In row 2, the actual FI profile is in blue, closely matching the target profile in gray.</figcaption> </figure> <p>The <strong>second point</strong> I am going make will be of no consequence under the natural image statistics that we typically care about, but may provoke some thoughts about what may happen in a strange world. Going back to the derivation from \(1\) to \(6\), we have glossed over an important assumption that in fact enabled us to link efficient orientation encoding and the fundemental principle of preserving maximal information content from an image in the first place.</p> <p>In \eqref{eq1} it may have appeared that \(H[r\\|\theta]\) is only a function of \(\theta\) but not other visual features. But as we know, a V1 neuron is selective for both orientation and other features such as spatial frequency. This means that the firing rate \(r\) simultaneously or jointly encodes information about both \(\theta\) and frequency \(f\).</p> <p>To see why this could matter in pathological cases, we rewrite \eqref{eq1} as:</p> \[\begin{align} I[\theta, r] &amp;= H[r] - \int{\left( \int{H[r|\theta, f]p(f|\theta)df}\right)p(\theta)d\theta}\label{eq7}\tag{7} \end{align}\] <p>Note that \eqref{eq7} is only equal to \eqref{eq1} when we assume that either \(H[r\\|\theta,f]\) is independent of \(f\) or \(p(f\\|\theta)\) is independent of \(\theta\), the latter of which turns out in fact to be true. (As a side note, \(p(\theta\\|f)\) is also independent of \(f\). See the following joint power spectrum in figure 3.)</p> <figure> <p align="middle"> <img src="http://localhost:4000/assets/posts/efficient-coding-orien/joint_prior.png" width="40%" /> </p> <figcaption>Fig 3. Joint power spectrum of frequency and orientation for natural scenes, adapted from Torralba &amp; Oliva. The amplitude spectrum can be seen as a proxy for the prior distribution in natural scenes.</figcaption> </figure> <p>But what would happen to the efficient coding solution for \(J(\theta)\) if, hypothetically, both are untrue? For example, \(H[r\\|\theta,f]\) could depend on \(f\) for reasons such as redundancy reduction, and once we find a world where \(p(f\\|\theta)\) somehow is drastically different around some orientation, the efficient coding strategy for \(\theta\) will no longer have a simple dependence on \(p(\theta)\). Such a strange world is certainly not impossible to find.</p> </div> </article> </main> <small class="post-updated-at">updated_at 26-06-2024</small> <nav class="post-nav"> <a class="post-nav-item post-nav-prev" href="/noise-and-fisher-info/" > <div class="nav-arrow">Previous</div> <span class="post-title">Noise and two simplified forms of Fisher Information</span> </a> <a class="post-nav-item post-nav-next" href="/contrast-sensitivity/"> <div class="nav-arrow">Next</div> <span class="post-title">What do the retina and V1 know about spatial frequencies in natural scenes?</span> </a> </nav> <footer class="footer"> <!-- <a class="footer_item" href="javascript::void(0)"></a> --> <a class="footer_item"><!--<p xmlns:cc="http://creativecommons.org/ns#" >-->This blog is licensed under<a href="https://creativecommons.org/licenses/by-nc/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1" alt=""></a></a> <a class="footer_item" href="/nowhere"><br>RSS subscribe &nbsp; | &nbsp; Report a bug &nbsp; | &nbsp; Start a collaboration<br></a> <span class="footer_item">&copy; 2023-2024 Hanqi Zhang</span> <small class="footer_copyright"> <!-- Klisé Theme: https://github.com/piharpi/jekyll-klise --> <a href="https://github.com/piharpi/jekyll-klise" target="_blank" rel="noreferrer noopener" >klisé</a > theme on <a href="https://jekyllrb.com" target="_blank" rel="noreferrer noopener" >jekyll</a > </small> </footer> <script src="/assets/js/main.js" defer="defer"></script><!-- Google tag (gtag.js) --> <script async src="https://www.googletagmanager.com/gtag/js?id=3nddtc5_7VVzxN8M3C5cgZ3n3XJTgz0YeKDbSaW3cJc" ></script> <script> window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag("js", new Date()); gtag("config", "3nddtc5_7VVzxN8M3C5cgZ3n3XJTgz0YeKDbSaW3cJc"); </script> </div> </body> </html>
